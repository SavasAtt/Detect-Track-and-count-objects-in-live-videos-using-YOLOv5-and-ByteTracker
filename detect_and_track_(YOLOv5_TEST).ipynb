{"cells":[{"cell_type":"markdown","source":["#Importing the necessary libraries"],"metadata":{"id":"wZyfEdN19rm9"},"id":"wZyfEdN19rm9"},{"cell_type":"code","execution_count":1,"id":"6d392f1c","metadata":{"id":"6d392f1c","executionInfo":{"status":"ok","timestamp":1676218649979,"user_tz":-180,"elapsed":3080,"user":{"displayName":"Sava≈ü ATTARZADE","userId":"15757011517757620061"}}},"outputs":[],"source":["import cv2\n","import os\n","import numpy as np\n","import sys\n","import glob\n","import time\n","import torch\n","from google.colab import files\n","from google.colab.patches import cv2_imshow\n","from IPython import display"]},{"cell_type":"markdown","source":["#Copy the project repository:"],"metadata":{"id":"KIJTXtN79y1n"},"id":"KIJTXtN79y1n"},{"cell_type":"code","source":["!git clone 'https://github.com/SavasAtt/Detect-Track-and-count-objects-in-live-videos-using-YOLOv5-and-ByteTracker'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7Ix5gDyHvf3U","executionInfo":{"status":"ok","timestamp":1676218663852,"user_tz":-180,"elapsed":7875,"user":{"displayName":"Sava≈ü ATTARZADE","userId":"15757011517757620061"}},"outputId":"cf0dbd17-9cf7-4329-f40c-acf286af5ea2"},"id":"7Ix5gDyHvf3U","execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'Detect-Track-and-count-objects-in-live-videos-using-YOLOv5-and-ByteTracker'...\n","remote: Enumerating objects: 13, done.\u001b[K\n","remote: Counting objects: 100% (13/13), done.\u001b[K\n","remote: Compressing objects: 100% (13/13), done.\u001b[K\n","remote: Total 13 (delta 2), reused 0 (delta 0), pack-reused 0\u001b[K\n","Unpacking objects: 100% (13/13), 37.81 MiB | 8.99 MiB/s, done.\n"]}]},{"cell_type":"markdown","source":["#Copy YOLOv5 repository and install the necessary libraries:"],"metadata":{"id":"wmPil-sN96lt"},"id":"wmPil-sN96lt"},{"cell_type":"code","source":["if not os.path.exists('yolov5'):\n","    !git clone https://github.com/ultralytics/yolov5.git\n","\n","%cd yolov5/\n","!pwd\n","!pip install -r requirements.txt\n","%cd ..\n","\n","!pip install -r https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt\n","\n","display.clear_output()"],"metadata":{"id":"Bk1vE3XYwxB5","executionInfo":{"status":"ok","timestamp":1676218682354,"user_tz":-180,"elapsed":18508,"user":{"displayName":"Sava≈ü ATTARZADE","userId":"15757011517757620061"}}},"id":"Bk1vE3XYwxB5","execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["# Install all necessary libraries and dependencies for TensorRT:\n","\n","install these libraries only if you going to use TensorRT because it wil take long time to be installed."],"metadata":{"id":"01GsqSoMI2NH"},"id":"01GsqSoMI2NH"},{"cell_type":"code","source":["!pip install nvidia-pyindex\n","!pip install nvidia-tensorrt\n","!pip3 install tensorrt-5.0.2.6-py2.py3-none-any.whl  \n","!pip3 install pycuda \n","\n","display.clear_output()"],"metadata":{"id":"E1cQvL4gbEmY","executionInfo":{"status":"ok","timestamp":1676218915351,"user_tz":-180,"elapsed":233006,"user":{"displayName":"Sava≈ü ATTARZADE","userId":"15757011517757620061"}}},"id":"E1cQvL4gbEmY","execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["# Copy the trained **YOLOv5s** model and convert it to **ONNX** and **TensorRT**:"],"metadata":{"id":"JR65zf8dJCMy"},"id":"JR65zf8dJCMy"},{"cell_type":"code","source":["import shutil\n","\n","shutil.copyfile('/content/Detect-Track-and-count-objects-in-live-videos-using-YOLOv5-and-ByteTracker/best_model_YOLOv5s.pt', \n","                '/content/best_model_YOLOv5s.pt')\n","\n","!`python /content/yolov5/export.py --weights /content/best_model_YOLOv5s.pt --include torchscript engine --device 0`"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vMZxXtjSPch_","executionInfo":{"status":"ok","timestamp":1676219145825,"user_tz":-180,"elapsed":181578,"user":{"displayName":"Sava≈ü ATTARZADE","userId":"15757011517757620061"}},"outputId":"76c848d3-65af-43ba-aac2-89c2713d0cca"},"id":"vMZxXtjSPch_","execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[34m\u001b[1mexport: \u001b[0mdata=yolov5/data/coco128.yaml, weights=['/content/best_model_YOLOv5s.pt'], imgsz=[640, 640], batch_size=1, device=0, half=False, inplace=False, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=17, verbose=False, workspace=4, nms=False, agnostic_nms=False, topk_per_class=100, topk_all=100, iou_thres=0.45, conf_thres=0.25, include=['torchscript', 'engine']\n","YOLOv5 üöÄ v7.0-97-gfa4bdbe Python-3.8.10 torch-1.13.1+cu116 CUDA:0 (Tesla T4, 15110MiB)\n","\n","Fusing layers... \n","Model summary: 157 layers, 7015519 parameters, 0 gradients, 15.8 GFLOPs\n","\n","\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from /content/best_model_YOLOv5s.pt with output shape (1, 25200, 7) (13.8 MB)\n","\n","\u001b[34m\u001b[1mTorchScript:\u001b[0m starting export with torch 1.13.1+cu116...\n","\u001b[34m\u001b[1mTorchScript:\u001b[0m export success ‚úÖ 1.5s, saved as /content/best_model_YOLOv5s.torchscript (27.2 MB)\n","\u001b[31m\u001b[1mrequirements:\u001b[0m YOLOv5 requirement \"onnx>=1.12.0\" not found, attempting AutoUpdate...\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tensorflow 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.3 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/, https://pypi.ngc.nvidia.com\n","Collecting onnx>=1.12.0\n","  Downloading onnx-1.13.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.5 MB)\n","     ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 13.5/13.5 MB 30.8 MB/s eta 0:00:00\n","Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.8/dist-packages (from onnx>=1.12.0) (1.21.6)\n","Collecting protobuf<4,>=3.20.2\n","  Downloading protobuf-3.20.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n","     ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1.0/1.0 MB 67.4 MB/s eta 0:00:00\n","Requirement already satisfied: typing-extensions>=3.6.2.1 in /usr/local/lib/python3.8/dist-packages (from onnx>=1.12.0) (4.4.0)\n","Installing collected packages: protobuf, onnx\n","  Attempting uninstall: protobuf\n","    Found existing installation: protobuf 3.19.6\n","    Uninstalling protobuf-3.19.6:\n","      Successfully uninstalled protobuf-3.19.6\n","Successfully installed onnx-1.13.0 protobuf-3.20.3\n","\n","\u001b[31m\u001b[1mrequirements:\u001b[0m 1 package updated per ['onnx>=1.12.0']\n","\u001b[31m\u001b[1mrequirements:\u001b[0m ‚ö†Ô∏è \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n","\n","\n","\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.13.0...\n","\u001b[34m\u001b[1mONNX:\u001b[0m export success ‚úÖ 10.3s, saved as /content/best_model_YOLOv5s.onnx (27.2 MB)\n","\n","\u001b[34m\u001b[1mTensorRT:\u001b[0m starting export with TensorRT 8.5.3.1...\n","/content/yolov5/export.py:271: DeprecationWarning: Use set_memory_pool_limit instead.\n","  config.max_workspace_size = workspace * 1 << 30\n","\u001b[34m\u001b[1mTensorRT:\u001b[0m input \"images\" with shape(1, 3, 640, 640) DataType.FLOAT\n","\u001b[34m\u001b[1mTensorRT:\u001b[0m output \"output0\" with shape(1, 25200, 7) DataType.FLOAT\n","\u001b[34m\u001b[1mTensorRT:\u001b[0m building FP32 engine as /content/best_model_YOLOv5s.engine\n","/content/yolov5/export.py:298: DeprecationWarning: Use build_serialized_network instead.\n","  with builder.build_engine(network, config) as engine, open(f, 'wb') as t:\n","\u001b[34m\u001b[1mTensorRT:\u001b[0m export success ‚úÖ 161.6s, saved as /content/best_model_YOLOv5s.engine (30.6 MB)\n","\n","Export complete (173.9s)\n","Results saved to \u001b[1m/content\u001b[0m\n","Detect:          python detect.py --weights /content/best_model_YOLOv5s.engine \n","Validate:        python val.py --weights /content/best_model_YOLOv5s.engine \n","PyTorch Hub:     model = torch.hub.load('ultralytics/yolov5', 'custom', '/content/best_model_YOLOv5s.engine')  \n","Visualize:       https://netron.app\n","/bin/bash: [02/12/2023-16:23:11]: No such file or directory\n"]}]},{"cell_type":"markdown","source":["## Install ByteTrack\n","\n","[ByteTrack](https://github.com/ifzhang/ByteTrack) is great tracker and we can use it with [YOLOv5]."],"metadata":{"id":"79YR0sLoUvLT"},"id":"79YR0sLoUvLT"},{"cell_type":"code","source":["HOME = '/content'\n","\n","%cd {HOME}\n","!git clone https://github.com/ifzhang/ByteTrack.git\n","!cd ByteTrack && pip3 install -q -r requirements.txt\n","!cd ByteTrack && python3 setup.py -q develop\n","!pip install -q cython_bbox\n","!pip install -q onemetric\n","\n",",\n","display.clear_output()\n","\n","import sys\n","sys.path.append(f\"{HOME}/ByteTrack\")\n","\n","\n","import yolox\n","print(\"yolox.__version__:\", yolox.__version__)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EmTErrFfUlom","executionInfo":{"status":"ok","timestamp":1676219223684,"user_tz":-180,"elapsed":77877,"user":{"displayName":"Sava≈ü ATTARZADE","userId":"15757011517757620061"}},"outputId":"4404f396-059d-4be0-95e6-4df4e2884982"},"id":"EmTErrFfUlom","execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["yolox.__version__: 0.1.0\n"]}]},{"cell_type":"code","source":["from yolox.tracker.byte_tracker import BYTETracker, STrack\n","from onemetric.cv.utils.iou import box_iou_batch\n","from dataclasses import dataclass"],"metadata":{"id":"juvl3lCAU0L_","executionInfo":{"status":"ok","timestamp":1676219224254,"user_tz":-180,"elapsed":581,"user":{"displayName":"Sava≈ü ATTARZADE","userId":"15757011517757620061"}}},"id":"juvl3lCAU0L_","execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["## Install Roboflow Supervision"],"metadata":{"id":"f5Dx1Px5U28f"},"id":"f5Dx1Px5U28f"},{"cell_type":"code","source":["!pip install -i https://test.pypi.org/simple/ supervision\n","\n","\n","display.clear_output()\n","\n","import supervision\n","print(\"supervision.__version__:\", supervision.__version__)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"V4Vgaej8U33F","executionInfo":{"status":"ok","timestamp":1676219229511,"user_tz":-180,"elapsed":5259,"user":{"displayName":"Sava≈ü ATTARZADE","userId":"15757011517757620061"}},"outputId":"af2f4edb-7a3e-4532-e658-13605ba33419"},"id":"V4Vgaej8U33F","execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["supervision.__version__: 0.2.0\n"]}]},{"cell_type":"code","source":["from supervision.draw.color import ColorPalette\n","from supervision.geometry.utils import Point\n","from supervision import VideoInfo\n","from supervision import get_video_frames_generator\n","from supervision import VideoSink\n","from supervision.notebook.utils import show_frame_in_notebook\n","from supervision import Detections, BoxAnnotator\n","# from supervision.tools.line_counter import LineCounter, LineCounterAnnotator"],"metadata":{"id":"9etCRRWEU-Nz","executionInfo":{"status":"ok","timestamp":1676219229511,"user_tz":-180,"elapsed":4,"user":{"displayName":"Sava≈ü ATTARZADE","userId":"15757011517757620061"}}},"id":"9etCRRWEU-Nz","execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["# Tracking utils and functions:"],"metadata":{"id":"nuPWb_RgU7X7"},"id":"nuPWb_RgU7X7"},{"cell_type":"code","source":["from typing import List\n","\n","import numpy as np\n","\n","\n","# converts Detections into format that can be consumed by match_detections_with_tracks function\n","def detections2boxes(detections: Detections) -> np.ndarray:\n","    return np.hstack((\n","        detections.xyxy,\n","        detections.confidence[:, np.newaxis]\n","    ))\n","\n","\n","# converts List[STrack] into format that can be consumed by match_detections_with_tracks function\n","def tracks2boxes(tracks: List[STrack]) -> np.ndarray:\n","    return np.array([\n","        track.tlbr\n","        for track\n","        in tracks\n","    ], dtype=float)\n","\n","\n","# matches our bounding boxes with predictions\n","def match_detections_with_tracks(\n","    detections: Detections, \n","    tracks: List[STrack]\n",") -> Detections:\n","    if not np.any(detections.xyxy) or len(tracks) == 0:\n","        return np.empty((0,))\n","\n","    tracks_boxes = tracks2boxes(tracks=tracks)\n","    iou = box_iou_batch(tracks_boxes, detections.xyxy)\n","    track2detection = np.argmax(iou, axis=1)\n","    \n","    tracker_ids = [None] * len(detections)\n","    \n","    for tracker_index, detection_index in enumerate(track2detection):\n","        if iou[tracker_index, detection_index] != 0:\n","            tracker_ids[detection_index] = tracks[tracker_index].track_id\n","\n","    return tracker_ids"],"metadata":{"id":"lL9HRijlVa9Q","executionInfo":{"status":"ok","timestamp":1676219229512,"user_tz":-180,"elapsed":4,"user":{"displayName":"Sava≈ü ATTARZADE","userId":"15757011517757620061"}}},"id":"lL9HRijlVa9Q","execution_count":10,"outputs":[]},{"cell_type":"markdown","source":["#Copy test video and make second copy of it:"],"metadata":{"id":"LDnvnzj2-XD1"},"id":"LDnvnzj2-XD1"},{"cell_type":"code","source":["import shutil\n","\n","src=\"/content/Detect-Track-and-count-objects-in-live-videos-using-YOLOv5-and-ByteTracker/test.mp4\"\n","dst=\"/content/Detect-Track-and-count-objects-in-live-videos-using-YOLOv5-and-ByteTracker/test2.mp4\"\n","\n","shutil.copy(src,dst)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"H-EDTw3jVxW1","executionInfo":{"status":"ok","timestamp":1676219263045,"user_tz":-180,"elapsed":383,"user":{"displayName":"Sava≈ü ATTARZADE","userId":"15757011517757620061"}},"outputId":"ac01e545-46f9-40cd-c08d-d02f91aa60d3"},"id":"H-EDTw3jVxW1","execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/content/Detect-Track-and-count-objects-in-live-videos-using-YOLOv5-and-ByteTracker/test2.mp4'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["# settings\n","# LINE_START = Point(0, 50)\n","# LINE_END = Point(640, 50)\n","%cd {HOME}\n","TARGET_VIDEO_PATH = src\n","TARGET_VIDEO_PATH2 = dst\n","VideoInfo.from_video_path(TARGET_VIDEO_PATH)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-fNxI0srWL9i","executionInfo":{"status":"ok","timestamp":1676219269137,"user_tz":-180,"elapsed":464,"user":{"displayName":"Sava≈ü ATTARZADE","userId":"15757011517757620061"}},"outputId":"249ed545-1061-409d-a807-c27d8a8e1ca6"},"id":"-fNxI0srWL9i","execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["/content\n"]},{"output_type":"execute_result","data":{"text/plain":["VideoInfo(width=640, height=640, fps=30, total_frames=1800)"]},"metadata":{},"execution_count":13}]},{"cell_type":"markdown","source":["#Setting parameters for ByteTracker:"],"metadata":{"id":"_Z-Fo-i4-i3l"},"id":"_Z-Fo-i4-i3l"},{"cell_type":"code","source":["@dataclass(frozen=True)\n","class BYTETrackerArgs:\n","    track_thresh: float = 0.7\n","    track_buffer: int = 100\n","    match_thresh: float = 0.8\n","    aspect_ratio_thresh: float = 3.0\n","    min_box_area: float = 10\n","    mot20: bool = False"],"metadata":{"id":"QDIqOl8cWQFy","executionInfo":{"status":"ok","timestamp":1676219271684,"user_tz":-180,"elapsed":1,"user":{"displayName":"Sava≈ü ATTARZADE","userId":"15757011517757620061"}}},"id":"QDIqOl8cWQFy","execution_count":14,"outputs":[]},{"cell_type":"markdown","source":["# Setting Model format to be **'PT'** or **'ONNX'** or **'TensorRT'**:"],"metadata":{"id":"pfsjtjK3-mSu"},"id":"pfsjtjK3-mSu"},{"cell_type":"code","source":["model_type = 'engine' # 'pt' or 'onnx' or 'engine'\n","best_model_path = f'/content/best_model_YOLOv5s.{model_type}'\n","model = torch.hub.load('ultralytics/yolov5', 'custom', best_model_path)\n","os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TWOwzenOMtA3","executionInfo":{"status":"ok","timestamp":1676219279805,"user_tz":-180,"elapsed":5328,"user":{"displayName":"Sava≈ü ATTARZADE","userId":"15757011517757620061"}},"outputId":"dc913d2d-eab0-41d2-aeef-b83bc3eeda2e"},"id":"TWOwzenOMtA3","execution_count":15,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.8/dist-packages/torch/hub.py:267: UserWarning: You are about to download and run code from an untrusted repository. In a future release, this won't be allowed. To add the repository to your trusted list, change the command to {calling_fn}(..., trust_repo=False) and a command prompt will appear asking for an explicit confirmation of trust, or load(..., trust_repo=True), which will assume that the prompt is to be answered with 'yes'. You can also use load(..., trust_repo='check') which will only prompt for confirmation if the repo is not already trusted. This will eventually be the default behaviour\n","  warnings.warn(\n","Downloading: \"https://github.com/ultralytics/yolov5/zipball/master\" to /root/.cache/torch/hub/master.zip\n","YOLOv5 üöÄ 2023-2-12 Python-3.8.10 torch-1.13.1+cu116 CUDA:0 (Tesla T4, 15110MiB)\n","\n","Loading /content/best_model_YOLOv5s.engine for TensorRT inference...\n"]},{"output_type":"stream","name":"stdout","text":["[Errno 2] No such file or directory: '/usr/local/lib/python3.8/dist-packages/setuptools-57.4.0.dist-info/METADATA'\n"]},{"output_type":"stream","name":"stderr","text":["Adding AutoShape... \n"]}]},{"cell_type":"markdown","source":["#Create **classes name dictionary** and class **IDs**:"],"metadata":{"id":"79wjTo8V_A1S"},"id":"79wjTo8V_A1S"},{"cell_type":"code","source":["# dict maping class_id to class_name\n","CLASS_NAMES_DICT = {0: 'Bolt', 1: 'Nut'}\n","# class_ids of interest - car, motorcycle, bus and truck\n","CLASS_ID = [0, 1]"],"metadata":{"id":"ASFxg0slVrKt","executionInfo":{"status":"ok","timestamp":1676219285550,"user_tz":-180,"elapsed":441,"user":{"displayName":"Sava≈ü ATTARZADE","userId":"15757011517757620061"}}},"id":"ASFxg0slVrKt","execution_count":16,"outputs":[]},{"cell_type":"markdown","source":["#Process the test video:\n","\n","Here the test video is processed and all required operations are done **(Detecting, tracking and counting)**. Also **live FPS, mean FPS and max FPS** is calculated and displayed on the video.\n","finally the processed video is dowonloaded."],"metadata":{"id":"_fzHVyJd_TNr"},"id":"_fzHVyJd_TNr"},{"cell_type":"code","source":["# create BYTETracker instance\n","byte_tracker = BYTETracker(BYTETrackerArgs())\n","# create VideoInfo instance\n","video_info = VideoInfo.from_video_path(TARGET_VIDEO_PATH)\n","# create frame generator\n","generator = get_video_frames_generator(TARGET_VIDEO_PATH)\n","# # create LineCounter instance\n","# line_counter = LineCounter(start=LINE_START, end=LINE_END)\n","# create instance of BoxAnnotator and LineCounterAnnotator\n","box_annotator = BoxAnnotator(thickness=4, text_thickness=1, text_scale=0.6)\n","# line_annotator = LineCounterAnnotator(thickness=4, text_thickness=4, text_scale=2)\n","\n","FPS_list = []\n","if model_type == 'pt':\n","  model_format = 'PT'\n","elif model_type == 'onnx':\n","  model_format = 'ONNX'\n","elif model_type == 'engine':\n","  model_format = 'TensorRT'\n","\n","Sink_container = VideoSink(TARGET_VIDEO_PATH2,video_info)\n","Sink_container.target_path = '/content/output_' + model_format + '.mp4'\n","\n","# open target video file\n","with Sink_container as sink:\n","    # loop over video frames\n","    for frame in generator:\n","        # model prediction on single frame and conversion to supervision Detections\n","\n","        start = time.perf_counter()\n","        results = model(frame)\n","        detections = Detections(\n","            xyxy=results.xyxyn[0][:, :-2].cpu().numpy() * video_info.width,\n","            confidence=results.xyxyn[0][:, 4].cpu().numpy(),\n","            class_id=results.xyxyn[0][:, -1].cpu().numpy().astype(int)\n","        )\n","        # print(detections)\n","        print('')\n","        # filtering out detections with unwanted classes\n","        mask = np.array([class_id in CLASS_ID for class_id in detections.class_id], dtype=bool)\n","        detections.filter(mask=mask, inplace=True)\n","        # tracking detections\n","        tracks = byte_tracker.update(\n","            output_results=detections2boxes(detections=detections),\n","            img_info=frame.shape,\n","            img_size=frame.shape\n","        )\n","        tracker_id = match_detections_with_tracks(detections=detections, tracks=tracks)\n","        detections.tracker_id = np.array(tracker_id)\n","        # filtering out detections without trackers\n","        mask = np.array([tracker_id is not None for tracker_id in detections.tracker_id], dtype=bool)\n","        detections.filter(mask=mask, inplace=True)\n","        # format custom labels\n","        labels = [\n","            f\"#{tracker_id} {CLASS_NAMES_DICT[class_id]} {confidence:0.2f}\"\n","            for _, confidence, class_id, tracker_id\n","            in detections\n","        ]\n","        # # updating line counter\n","        # line_counter.update(detections=detections)\n","        # annotate and display frame\n","        frame = box_annotator.annotate(scene=frame, detections=detections, labels=labels)\n","        \n","        end = time.perf_counter()\n","        totalTime = end - start\n","        fps = 1 / totalTime\n","        FPS_list.append(fps)\n","        min_fps = min(FPS_list)\n","        max_fps = max(FPS_list)\n","        mean_fps = round(sum(FPS_list)/len(FPS_list))\n","\n","        cv2.putText(frame, f'YOLO5s[{model_format}]', (20,30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255,255,255), 2)\n","        cv2.putText(frame, f'FPS: {int(fps)}', (20,60), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0,0,255), 2)\n","        cv2.putText(frame, f'Mean FPS: {int(mean_fps)}', (20,90), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255,0,0), 2)\n","        cv2.putText(frame, f'Max FPS: {int(max_fps)}', (20,120), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0,255,0), 2)\n","        cv2.putText(frame, 'GPU: ' + torch.cuda.get_device_name(0), (20,150), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255,255,0), 2)\n","\n","        sink.write_frame(frame)\n","# display.clear_output()\n","\n","print('Video process is done.')\n","print('Model: YOLO5s.' + model_format)\n","print('GPU: ' + torch.cuda.get_device_name(0))\n","print('Mean FPS:' + str(mean_fps))\n","print('Max FPS:' + str(max_fps))\n","print('')\n","print('Now downloading the processed video:')\n","files.download('/content/output_' + model_format + '.mp4') "],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"2etdnb9wUlrh","executionInfo":{"status":"ok","timestamp":1676219320342,"user_tz":-180,"elapsed":32771,"user":{"displayName":"Sava≈ü ATTARZADE","userId":"15757011517757620061"}},"outputId":"bb46bd5e-b93b-4725-9728-0b3e51826026"},"id":"2etdnb9wUlrh","execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","Video process is done.\n","Model: YOLO5s.TensorRT\n","GPU: Tesla T4\n","Mean FPS:82\n","Max FPS:123.34202113415016\n","\n","Now downloading the processed video:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_c21ad518-ad2d-477f-957e-656079d24e48\", \"output_TensorRT.mp4\", 12729263)"]},"metadata":{}}]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.1"},"colab":{"provenance":[]},"accelerator":"GPU","gpuClass":"standard"},"nbformat":4,"nbformat_minor":5}